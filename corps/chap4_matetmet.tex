Dans cette thèse nous avons developpés des méthodes basées sur \gls{ia} pour exploitée les données mutlimodales de patients. Pour cela nous avons utilisé un vaste panel de ressources biologiques, d'outils informatiques et de méthodes \gls{ia}. Dans ce chapitre nous allons décrire l'ensemble des outils et ressources utilisées pour construire nos méthodes d'analyse.

\chapter{Outils informatiques et données utilisées}

\section{Données biomédicales de myopathies congénitales}
Nous avons développé des méthodes \gls{ia} pour analyser des jeux de données multimodales de myopathies congénitale. Pour developper ces méthodes nous nous sommes basés sur des données d'imageries et des comtpes rendus de biopsie. La source de ces données sont présentées ci-dessous.

\subsection{Comptes rendus de biopsie de l'Institut de Myologie de Paris}
La première source de donnée provient de l'Institut de Myologie de Paris. Grâce à une collaboration avec l'équipe du laboratoire d’histopathologie d'abord dirigé par Norma B. Romero puis Teresinha Evangelista, nous avons pu récupérer et utiliser 192 comptes rendus de biopsie musculaire de patient atteint de myopathies (congénitales, dystrophies ou autre), dont 138 spécifiquement atteint par des myopathies congénitales identifiée. Ces rapports sous forme papier, ont été scannés puis anonymisés d'abord avec un outil d'anonymisation que nous avons développé (présenté dans le chapitre 7), puis vérifié à la main. La figure \ref{fig:compte-rendu-exemple} présente la structure d'un comptes rendus de biopsie anonymisé typique présent dans le jeu de donnée. Il y a deux type de comptes rendus, ceux qui concernent les observations en microscopie photoniques et ceux qui concernent les observations en microscopie électronique. Cependant cette structure peut varier en fonction de l'année de production du compte rendu, certains sont totalement déstructurés.

\begin{figure}[!htbp]
 \centering
 \includegraphics[width=0.9\textwidth]{figures/compte_rendu_exemple.png}
 \caption[Exemple de compte rendu de biopsie]{Exemple de compte rendu de biopsie en miscroscopie photonoqiue anonymisé de l'institut de myologie de Paris}
 \label{fig:compte-rendu-exemple}
\end{figure}

\subsection{Images de biopsie musculaire de souris}
Une seconde source de données provient d'une collaboration avec l'\gls{igbmc}, plus spécifiquement avec l'équipe Physiopathologie des maladies neuromusculaires dirigée par Jocelyn Laporte. Cette équipe travaille sur les myopathies congénitales et utilise plusieurs souris modèles de myopathies congénitales. Ainsi en travaillant avec les membres de l'équipe réalisant des biopsies musculaire sur ces modèles, nous avons pu récupérer des développer des méthodes d'analyse pour des biopsie musculaire de souris aux colorations \gls{he}, \gls{sdh}, ATPase et à fluorescence.

\section{Ontologies biologiques}
En biologie, les ontologies sont des vocabulaires standards pour faciliter l'intégration des données et leur analyse. Dans cette thèse, pour standardiser les données issues des comptes rendus, notamment dans le cadre du chapitre sur \gls{impatient} et l'analyse de sa base de données, nous avons utilisés diverses ontologies pré-existantes que nous allons décrire ici.

\subsection{Ontologie des phénotypes: HPO}
L'ontologie \gls{hpo}, developpée en 2008 par Peter N Robinson et Sebastian Köhler au \textit{Charité University Hospital} à Berlin (\cite{robinson_human_2008}, \cite{kohler_human_2021}), rassemble l'ensemble des phénotypes médicaux observables chez l'Homme. Organisée sous forme d'arbre, elle contient plus de 13 000 termes organisé selon un niveau croissant de précision (le terme "anomalie de l'oeil" est un parent du terme "anomalie de la pupille"). Chaque terme est associé à un identifiant unique sous la forme HPO:XXXXX et possède un certain nombre dannotations comme des maladies associés, des gènes associés, des synonymes, des publications associées. L'ensemble de ces informations est disponible en ligne sur le portail  \href{https://hpo.jax.org/}{https://hpo.jax.org/} qui permet aussi de télécharger l'ontologie dans les format standard (JSON, OBO, OWL). Cette ontologie est utilisée dans \gls{impatient} pour normaliser les observations cliniques des patients.

\subsection{Ontologie de maladies: ORDO par Orphanet}
L'\gls{ordo} est née d'une collaboration entre Orphanet (\href{https://www.orpha.net/}{https://www.orpha.net/}, \cite{maiella_orphanet_2013}), et l'Institut européen de bioinformatique (EBI). Orphanet est une ressource informatique ayant pour but de répertorier l'ensemble des informations concernant les maladies rares et les médicaments orphelins. L'ontologie ORDO dans Orphanet, répertorie plus de 7000 maladies rares connues. Chaque maladie rare est répertoriée sous un identifiant unique de la forme ORPHA:XXXXXX, par exemple la maladie "Myopathie congénitale sévère à némaline" correspond à l'identifiant ORPHA:171430. De plus chaque maladie est associées à des annotations tels que leur prévalence, des synonymes, un mode d'hérédité, un age d'appartion, un pronostic, les gènes causant la maladie et autre. De plus chaque maladie est liée à des symptômes cliniques grâce à un lien direct vers des identifiant de l'ontologie \gls{hpo}. Dans le cadre de notre outil \gls{impatient} nous avons utilisé cette ontologie pour normaliser le diagnostic final des patients.

\subsection{Nomenclature génétique: HGNC et HGVS}
L'ontologie HUGO (\href{https://www.genenames.org/}{https://www.genenames.org/}, acronyme de Human Genome Organisation, est gérée par le Comité de Nomenclature des Gènes de HUGO (HGNC) à l'Institut Européen de Bioinformatique. Ce comitée est responsable de l'attribution de noms uniques pour les gènes humains, que ce soit des gènes condant pour des protéines, gènes non-codants et pseudogènes. Au total c'est plus de 43000 nom de gènes uniques qui sont référencés et annotés avec des références vers des bases de données externes (banque de séquences, ortologie, banque de mutations, banque de structures, Orphanet...). Concernant les variations génétiques (mutations), c'est la nomemclature établis par la \gls{hgvs} (\href{https://www.hgvs.org/}{https://www.hgvs.org/}) qui fait autorité. Cette nomemclature spécifique la façon de représenter textuellement un variant génétique. Par exemple la notation selon cette nomemclature "LRG\_199p1:p.Trp24Cys" indique une mutation faux-sens dans la protéine LRG\_199 où l'aicde aminé n°24 est substitué d'un tryptophane à une cystéine. L'ontologie HUGO et la nomenclature HGVS sont toute deux intégrées dans \gls{impatient} pour normaliser le diagnostic génétique des patients (gène muté et localisation de la mutation responsable de la maladie).

\section{Développement de modèle de ML traditionnels et xAI}
Dans le cadre de l'analyse de la base de données de \gls{impatient} et des résultats d'\textit{embedding} de \gls{nlmyo} nous avons utilisé des algorithmes de machine-learning traditionels pour créer des modèles prédictifs. Dans cette section nous présentons les principaux outils utilisés en termes d'algorithmes et d'analyse de performances.

\subsection{\textit{Scikit-Learn}: une boite à outil pour l'apprentissage automatique}
\textit{Scikit-Learn} (\cite{pedregosa_scikit-learn_2011}, \href{https://scikit-learn.org/}{https://scikit-learn.org/}) est une bibliothèque de code python mettant à disposition un grand nombres d'outils permettant de préparer les données, d'entraîner des modèles de \textit{machine-learning} et d'évaluer ses performances. \textit{Scikit-learn} inclut des algorithmes de classification, de regression, de clustering, de réduction de dimensionalités, et d'optimisaitons et selection de modèles. Scikit-learn a été utilisé dans cette thèse pour: (i) partionner et normaliser les données, (ii) entrainer des modèles de classifications de différentes familles d'algorithmes (iii) optimiser et évaluer les performances des modèles.

\subsection{Validation croisée et évaluation des performances}
La validation croisée (\textit{cross-validation }en anglais) est une technique d'évaluation de modèle prédictifs permettant d'avoir une estimation plus robuste et précise des métriques de performances. Cette méthode est particulièrement utile pour le jeu de données de petite taille. La figure \ref{fig:cross-val} présente schématiquement son fonctionnement. Le jeu de donnée initial est séparé en N sous-ensemble (nommés \textit{folds}, ici au nombre de 5). Ensuite le modèle est entraîné sur l'ensemble des sous ensemble a l'exception de un utilisé comme jeu de teste des performances. Ce processus est répété autant que de fois qu'il n'y a de fold afin que chaque fold ai été utilisé exactement une fois comme jeu de test. Ainsi pour 5 folds, cinq modèles sont entrainés et évalués. On peut alors calculer une performance moyenne du modèle entraîné sur l'ensemble des données, en calculant les performances moyenne des cinq modèles. Plus le nombre de fold est important, plus cette moyenne est précise mais c'est plus couteux en temps et ressource de calcul car il faut entrainer plus de modèles.
\begin{figure}[!htbp]
 \centering
 \includegraphics[width=0.9\textwidth]{figures/cross-val.png}
 \caption[Schéma validation-croisée]{Schéma du principe validation-croisée (modifié de la documentation de \textit{Scikit-Learn})}
 \label{fig:cross-val}
\end{figure}
\subsection{Rechercher d'hyper-paramètres}
La recherche d'hyper-paramètres est une étape importante dans le developpement de modèle prédictifs pour améliorer les performances des modèles. Les hyper-paramètres sont des paramètres propre à chaque algorithm, qui sont spécifiés en amont de l'entrainnement et qui ne varient pas lors de l'entrainnement mais qui influent dirrectement sur les performances du modèle. Par exemple, dans le cadre de l'entrainnement d'un arbre de décision, un exemple d'hyper-paramtère peut être la profondeur maximal de l'arbre ou encore la méthode de calcul de qualité d'un noeud (gini ou entropie).
\begin{figure}[!htbp]
 \centering
 \includegraphics[width=0.3\textwidth]{figures/parameters_grid.png}
 \caption[Schéma rechercher hyper-paramètres]{Schéma du principe de recherche d'hyper-paramètres par grille pour 2 paramètres. Un point rouge représente un modèle entraîné. }
 \label{fig:params_grid}
\end{figure}
La recherche d'hyper-paramtères reviens à trouver une combinaison de paramètre optimale qui maximise les perforances du modèles après entrainenement. La méthode la plus classique pour celà est l'optimisation par grille. A partir d'un ensemble de valeur possible pour chaque paramètres, on va entrainner et tester les performances du modèle pour chaque combinaisons de valeur et sélectionner sélectionner la combinaison de valeur la plus performante. La figure \ref{fig:params_grid}  présente un exemple théorique d'optimisation par grille pour deux paramètres. Pour chacun des paramètres 3 valeurs sont possibles, c'est donc un total de 9 modèles qui sont entrainés et testé. On peut alors mesurer l'impact de chaque paramtères sur les performances finales du modèle pour trouver quel paramètre est le plus important et quelle est sa valeure optimale.
D'autre méthodes moins naïve et moins coûteuse que la grille existent pour la recherche d'hyper-paramètre tel que l'approche bayésienne utilisée par la bibliothèque de code \textit{Optuna} (\href{https://optuna.org/}{https://optuna.org/}) qui permet de trouver une combinaison optimale plus rapidement.

\subsection{Algorithme de système de classeurs: ExSTraCS 2.0}
Les systèmes de classeurs sont une famille d'algorithmes de \gls{ml} considérés comme explicables. Dans cette thèse, nous avons utilisé le \gls{lcs} nommé \textit{ExSTraCS 2.0} (\cite{urbanowicz_exstracs_2015}), un algorithme de \gls{lcs}, spécifiquement développé pour les tâches de classifications à partir de données complexes, hétérogènes et de haute dimensionalité. Cet algorithme a été utilisé pour tenter de prédire le diagnostic de sous-type de myopathies congénitales des patients à partir des annotations réalisée dans \gls{impatient}. L'implémentation en python de cet algorithme de \gls{lcs} nommée \textit{scikit-ExSTraCS} (\href{https://github.com/UrbsLab/scikit-ExSTraCS}{https://github.com/UrbsLab/scikit-ExSTraCS}) nous a permis d'entraîner un modèle basé sur cette méthode et de comparer ses performances aux autres algorithme implémentés dans \textit{scikit}, via le pipeline d'entraînement et de comparaison de modèles nommé \textit{Streamline}.

\subsection{Streamline: un pipeline d'entraînement et de comparaison de modèles ML}
Dans le cadre de l'analyse de la base de données d'\gls{impatient}, nous avons voulu entrainer le modèle de classification (prédiction de diagnostic) le plus performant possible et de manière reproductible. Pour cela nous avons utilisé le pipeline d'entrainnement et de comparaison de modèle de machine-learning nommé \textit{Streamline} (\cite{urbanowicz_streamline_2023}). La figure \ref{fig:streamline} présente son fonctionnement. Il y a 4 étapes principales. L'étape 1 (module 1 et 2) consiste à la lecture, exploration statistique et préparation des données. L'étape 2 (module 3 et 4), consiste au calcul de l'importance de chaque annotation (comprendre ici: \textit{feature}) et de la sélections des annotations les plus pertinentes pour la classification. L'étape 3 (module 5) consiste à l'optimisation et l'entraînement de 16 algorithme de \gls{ml} variés et l'évaluation des performances des modèles issus de cet entraînement. Finalement l'étape 4 (module 6, 7, 8 ,9) correspond à la phase de post-traitement, où seront générées les figures de comparaisons des performances des modèles et le nettoyage des fichiers. STREAMLINE n'est pour l'instant disponible que pour faire de la classficiation binaire. Nous nous intéressant à un problème de classification multi-classe, donc nous avons modifié le pipeline pour le rendre compatible avec nos données. Le code du pipeline modifié est accessible à l'adresse \href{https://github.com/lambda-science/STREAMLINE}{https://github.com/lambda-science/STREAMLINE}.
\begin{figure}[!htbp]
 \centering
 \includegraphics[width=1\textwidth]{figures/STREAMLINE_paper_lightcolor.png}
 \caption[Pipeline STREAMLINE]{Schéma du pipeline STREAMLINE}
 \label{fig:streamline}
\end{figure}
\subsection{Métriques de performances}
Pour évaluer les performances de nos modèles, nous avons utilisés plusieurs métriques. Dans le cadre d'une classification multi-classe et déséquilibrée, la mesure de l'exactitude de classification (\textit{accuracy}) n'est pas suffisante pour obtenir une bonne représentation des performances des modèles. Ainsi nous mesurons aussi l'exactitude équilibrée (\textit{balanced accuracy}), le score F1, score F1 macro, spécificité et le coefficient de corrélation de Matthew. L'ensemble de ces métriques de performances sont calculées à partir de la matrice de confusion

\subsubsection{Matrice de confusion}
La matrice de confusion est une matrice en deux dimesisons qui compare pour un modèle les prédictions du modèles par rapport aux annotations réel des données. Par exemple pour une classifications en 3 classes, un matrice de confusion de taille (3x3) est produite. La diagonale (haut gauche vers bas droite) représente l'ensemble des prédicitons correctes (vrai positifs et vrai négatifs) c'est à dire les points pour lesquels la prédiction est en accord avec l'annotation réel. Les autres cases représentent l'ensemble des prédictions incorrectes (faux positifs ou faux négatifs). La figure \ref{fig:confusion-example}  représente un exemple de matrice de confusion pour une classification binaire en 2 classes.
\begin{figure}[!htbp]
 \centering
 \includegraphics[width=0.5\textwidth]{figures/confusion_example.png}
 \caption[Exemple de matrice de confusion binaire]{Exemple de matrice de confusion binaire}
 \label{fig:confusion-example}
\end{figure}

\subsubsection{Exactitude}
L'exactitude est une mesure de performance classique qui évalue la proportion de points de données correctéement classée par rapport aux nombre total de données. Cette mesure est trompeuse dans le cadre de données déséquilibrées. Elle est calculée tel que:
\[ \text{Exactitude} = \frac{\text{Vrais Positifs} + \text{Vrais Négatifs}}{\text{Vrais Positifs} + \text{Vrais Négatifs} + \text{Faux Positifs} + \text{Faux Négatifs}} \]

\subsubsection{Exactitude équilibrée}
L'exactitude équilibrée quant à elle est utile dans le cadre d'un jeu de données déséquilibré. Elle donne une importance égale aux performances de chaque classe. Pour cela, la sensibilité (ou rappel, c'est-à-dire le nombre de bonne prédiction pour une classe sur l'ensemble des membres de la classe), est calculée pour chaque classe. Ensuite l'exactitude équilibrée correspond donc à la moyenne de la sensibilité pour chaque classe. Ainsi a formule est égale à:
\[ \text{Exactitude équilibrée} = \frac{1}{n} \sum_{i=1}^{n} \text{Exactitude}_i \] où n représente le nombre de classe différente.

\subsubsection{Précision, rappel, spécificité et Score F1}
La figure \ref{fig:prec_recall_spec} présente les notions de précision, sensibilité et spécificité (rappel) en prenant pour exemple l'interprétation d'un test COVID. Pour un test COVID, la précision représente la proportion de patient réellement positifs parmi des test positifs. Le rappel (ou sensibilité) représente la proportion de test positif parmi l'ensemble des personnes positives à la COVID-19. Enfin la spécificité mesure la proportion de tests réellement négatif parmi l'ensemble des patients négatifs au COVID-19.

\begin{figure}[!htbp]
 \centering
 \includegraphics[width=1\textwidth]{figures/prec_recall.png}
 \caption[Précision, sensibilité et spécificité]{Schéma de la notion de précision, sensibilité et spécificité (modifié de Wikipédia)}
 \label{fig:prec_recall_spec}
\end{figure}

Le score-F1 correspond à la moyenne harmonique de la précision et de la sensibilité et donc permet de tenir compte à la fois des faux positifs et des faux négatifs. Dans le cadre de classification multi-classe (3 et plus), e score F1 peut être calculé de plusieurs manière. Soit de manière "micro" c'est à dire globalement à partir du nombre de vrai positifs, faux négatifs et faux positifs. Soit de manière "macro", en calculant le score F1 de chaque classe et en réalisant leur moyenne, similairement à la différence entre exactitude et exactitude équilibrée. Ainsi la formule du score F1-Macro est:
\[ \text{Score F1 Macro} = \frac{1}{n} \sum_{i=1}^{n} \frac{2 \times \text{Précision}_i \times \text{Rappel}_i}{\text{Précision}_i + \text{Rappel}_i} \] où \(n\) représente le nombre de classe différente.

\subsubsection{Coefficient de corrélation de Matthew}
La coefficient de corrélation de Matthew \textit{(Matthew Correlation Coefficient, MCC)}, est une métrique prenant en compte l'ensemble des éléments de la matrice de confusion (vrai positifs, faux positifs, vrai négatifs, faux négatifs), contrairement aux métriques présentées ci-dessus. De plus elle est équilibrée, c'est à dire qu'elle n'est pas biaisée dans le cas de de classes déséquilibrées. Ses valeurs sont comprises entre -1 et 1, avec 0 pour des prédictions aléatoires, 1 pour des prédictions parfaites et -1 pour des prédictions totalement contraires. Cette métrique est plus informative sur la qualité d'un modèle que le score F1 (\cite{chicco_advantages_2020}). Dans le cadre d'une classification binaire, la formule du MCC est:
\[ MCC = \frac{VP \times VN - FP \times FN}{\sqrt{(VP + FP)(VP + FN)(VN + FP)(VN + FN)}} \]
Dans le cadre d'une classification multi-classe, la formule est plus complexe:
\[ MCC = \frac{
    c \times s - \sum_{k}^{K} p_k \times t_k
}{\sqrt{
    (s^2 - \sum_{k}^{K} p_k^2) \times
    (s^2 - \sum_{k}^{K} t_k^2)
}} \]
où \(c\) représente le nombre d'échantillon correctement prédis, \(s\) représente le nombre total d'échantillon, \(p_k\) le nombre de fois où la classe k a été prédite, \(t_k\) le nombre de fois où la classe k s'est réellement produite

\section{Techniques d'analyse d'images et réseaux de neurones}
Les méthodes que nous avons developpés permettent d'analyser des données d'imageries histologiques. L'outil \gls{impatient} présenté dans la chapitre 5 permet de faire de l'annotation et segmentaiton d'images en utilisant des techniques d'analyse d'image traditionnelles. L'outil \gls{myoquant} présenté dans le chapitre 8 permet de faire de la quantification de marqueurs pathologiques grâce à la fois à des méthodes d'analyse traditionnelles mais aussi grace à des modèles pré-entrainés et des réseuax de neuronnes profonds.  Dans cette section nous allons voir quels sont les bibliothèques de codes, modèles et matériel que nous avons utilisé. 

\subsection{Méthode d'analyse d'image traditionnelles avec scikit-image}
Pour l'analyse d'image en utilisant des méthodes tradiotionelles nous avons utilisé la bibliothèque de code \textit{scikit-image} (\cite{walt_scikit-image_2014}, \href{https://scikit-image.org/}{https://scikit-image.org/}). \textit{Scikit-image} a été développé en 2014 et met à disposition des outils de base pour l'analyse d'image tel que le calcul de contraste, d'intensité et de texture de pixels, qui sont des mesures utilisées par \gls{impatient} pour la segemntation d'image. Dans \gls{myoquant}, \textit{scikit-image} est utilisé pour tracer des lignes, réaliser de l'érosion d'image et mesurer des surfances, périmètres, diamètre de Feret et la position des centroide à partir de masques de segmentation.

\subsection{Modèle pré-entrainé Cellpose et Stardist}
Dans le cadre de \gls{myoquant} nous avons utilisés des modèles pré-entrainé pour l'analyse des images de coupes histologiques. Pour la segmentation des fibres musculaire, nous avons utilisé l'implémentation python du modèle Cellpose (\cite{stringer_cellpose_2021}, \href{https://github.com/MouseLand/cellpose}{https://github.com/MouseLand/cellpose}). Parmi les différents modèles inclut dans Cellpose, nous avons utilisé spécifiquement le modèle \textit{cyto2}, qui est le modèle disponible dans Cellpose le plus récent et performant pour la segmentation des fibres musculaire sous diverses colorations. 

Quant à la segmentation des noyaux cellulaires, nous avons utilisé l'implémention python du modèle Stardist (\cite{weigert_star-convex_2020}, \href{https://github.com/stardist/stardist}{https://github.com/stardist/stardist}). Plus précisément dans le cadre de l'analyse des noyaux dans la coloration \gls{he}, nous avons utilisé le modèle pré-entrainé \textit{2D\_versatile\_he}, qui est un modèle spécifiquement entrainé sur des images à la coloration \gls{he}.

\subsection{Développement de réseau de neurones profond de type ResNet avec Keras Tensorflow}
En plus des modèles pré-entrainés, dans \gls{myoquant} nous avons entraîné et intégré nos propres modèles de classification basé sur les réseaux de neuronnes profonds. Pour cela nous avons utilisé les bilbiothèques de code Keras (\cite{chollet_keras_2015}, \href{https://keras.io/}{https://keras.io/}) et Tensorflow (\cite{martin_abadi_tensorflow_2015}, \href{https://www.tensorflow.org/}{https://www.tensorflow.org/}). Keras est une bibliothèque qui permet une intéraction simplifiée avec Tensorflow mettant à disposition plusieurs architectures de modèles pré-établis ainsi que des fonctions permettant d'accélérer le développement de modèle de réseaux de neuronnes. Dans le cadre de développement de modèle de classification d'image nous avons utilisé l'architecture RestNet50 version 2 pré-entrainée sur le jeu de données \textit{ImageNet} implémentée dans Keras. L'architecture ResNet50 (\cite{he_deep_2015}) est un réseau de neuronnes convolutifs composé de 48 couches convolutives. Ce modèle possède un total 23,564,800 paramètres. L'entrainnement de ce modèle et son inférence sur des données d'imageries ont été réalisé sur des machine-virtuelle de la plateforme SCIGNE Grand-Est équipée de \gls{gpu} RTX 2080 Ti.

\section{Développement d'outils basés sur modèles linguistiques de grande taille}
Les méthodes que nous avons developpé (\gls{nlmyo}) pour analyser du texte libre (comptes rendu de biopsie) se basent sur les modèles linguistiques de grande taille. Dans cette section, nous détaillons quels sont les outils et modèles que nous avons utilisé.

\subsection{Reconnaissance de texte avec Tesseract}
Dans un premier temps, comme la majorité des comptes rendus sont au format PDF il est nécessaire de les convertir en texte. Pour cela nous avons utilisés des méthodes d'\gls{ocr}. L'outil libre que nous avons utilisé pour cela se nomme Tesseract (\cite{ray_tesseract_2015}) version 5, mis à disposition en novembre 2021. Cet outil est capable de reconnaître de manière robuste du texte dactylographié dans plus de 100 langues différentes.

\subsection{Modèles linguistiques de grande tailles utilisés}
Dans la cadre du développement de \gls{nlmyo} nous avons utilisés 2 \gls{llms} génératifs et 2 \gls{llms} d'embedding. Pour chaque catégorie de \gls{llms} nous avons voulu comparer les résultats issues de modèles provenant de fournisseurs externes (OpenAI) par rapport à des modèles plus petits hébergés localement.

\subsubsection{Modèles génératifs: OpenAI GPT-3.5 et Vicuna-7B}
Un des critères déterminant pour le choix de modèle génératifs sont les performances du modèle et la taille de contexte. La taille de contexte pour un \gls{llms} représente le nombre de mots qu'il est capable de traiter lors d'une requête. Ainsi plus la taille de contexte est grande, plus il est possible d'analyser un document de grande taille avec des instructions détaillées. Par exemple, le modèle GPT-3.5-turbo d'OpenAI a une taille de contexte de 4096 jetons, ce qui représente environ 3000 mots en anglais. 

La grand majorité des modèles open-source et auto-hébergeable ont une taille de contexte de 512, ce qui limite leur utilisation pour l'analyse de grand documents. Notre choix de modèle auto-hébergeable s'est porté sur le modèle Vicuna-7B-1.1 (\cite{chiang_vicuna_2023}, \href{https://huggingface.co/vicuna/ggml-vicuna-7b-1.1}{https://huggingface.co/vicuna/ggml-vicuna-7b-1.1}), qui en plus d'être un modèle de petite taille (7 miliards de paramètres) donc facillement hébergeable, possède une taille de contexte de 2048. De plus ce modèles est le plus actuellement performant parmi les modèles open-source de taille 7B (\cite{hendrycks_measuring_2021}). 

Concernant le modèle provenant d'un fournisseur externe, nous avons choisis d'utiliser GPT-3.5-turbo d'OpenAI car ce modèle allie à la fois une  grande taille de contexte de 4096, d'excellentes performances (4e modèle le plus performant \cite{lianmin_zheng_chatbot_2023}) et possède une \gls{api} accessibles à bas coûts (0,002\$ par tranche de 1000 \textit{tokens}).

Pour finir, pour les deux modèles génératifs nous avons mis le paramètre de température le plus proche de 0 possible, c'est-à-dire à 0.01. Le paramètre de température contrôle le niveau de hasard des réponses du modèle. Plus ce paramètre est proche de 0 plus le réponse du modèle est déterministe. Plus cette valeur est supérieure à 1, plus la sortie est aléatoire. La valeur par défaut du modèle est 1. Ainsi en mettant ce paramètre très proche de 0 les résultats sont reproductibles. 

\subsubsection{Modèle d'\textit{embedding}: OpenAI et Instructor}
Les modèles linguistiques de grand taille d'embedding sont des modèles qui transforment du texte en vecteur numérique, ce qui permet de faire de la classification, du clustering et de la recherche de similarité. Ces modèles sont utilisés dans \gls{nlmyo} pour faire de la prédiction de diagnostic et pour créer un moteur de recheche de patient.

Comme pour les modèles  génératifs nous avons utilisés deux modèles, un par un fournisseur externe et un auto-hébergé pour comparer les performances. En terme de choix de modèles, pour le modèle issue de fournisseur externe nous avons utilisé le modèle d'\textit{embedding} nommé text-embedding-ada-002 car nous utilisons déjà le modèle génératif du même fournisseurs (OpenAI). De plus ce modèle est multi-langue et performant car il est classé 6e en terme de performances d'\textit{embedding} sur 75 modèles testés à travers un panel de 56 jeux de données (\cite{muennighoff_mteb_2022}).

Concernant le modèle auto-hébergé, nous avons choisis d'utiliser le modèle nommé Instructor (\cite{su_one_2023}, \href{https://huggingface.co/hkunlp/instructor-large}{https://huggingface.co/hkunlp/instructor-large}). Ce modèle est parmi les plus performant, classé 2e sur 75 modèles testés (\cite{muennighoff_mteb_2022}), de plus il comporte dans son jeu d'entrainnement des données issues de textes médicaux, ce qui permet d'espérer de bonne performances pour nos comptes rendus de biopsies. 

\subsection{Interaction avec les modèles linguistique de grande taille avec LangChain}

Face à la diversité des \gls{llms} génératifs, \gls{llms} d'\textit{embedding} et des outils associés, il est nécessaire d'avoir un outil qui uniformise la façon d'intéragir avec ces modèles. C'est l'objectifs de la bibliothèques de code nommée LangChain (\cite{chase_harrison_langchain_2022}) que nous avons utilisé à travers \gls{nlmyo}. Cette bibliothèque de code permet d'unifier la façon d'intéragir avec les modèles auto-hébergé et les différents fournisseurs externes, ce qui permet d'accélerer la phase d'exploration des performances des modèles et le developpement d'outil basés sur les \gls{llms}. 

En plus des interactions avec les modèles de language, LangChain mets à disposition des outils pour intéragir avec des bases de données optimisées pour le stockage et la requête de vecteur numériques (résultats des modèles d'\textit{embedding}). Ainsi dans \gls{nlmyo} nous avons utilisé la base de données de vecteur nommée ChromaDB (\href{https://www.trychroma.com/}{https://www.trychroma.com/}). Les bases de données de vecteurs, sont des base de données qui permettent de stocker des documents ainsi que leur résultats d'\textit{embedding}. Ces bases sont optimisée pour le calcul de similarité entre un vecteur requête et une grand base de données de vecteurs. Cela permet par exemple de construire des moteurs de recherches de documents, comme par exemple un moteur de recherches de comtpes-rendus de biposie requêtable par symptomes.

\section{Développement d'outils et d'interfaces}
Les divers outils que nous avons développé dans cette thèse (\gls{impatient}, \gls{nlmyo}, \gls{myoquant}) sous disponibles sous différentes forme: applications web complète, démonstration en ligne, outils en ligne de commande.  Pour cela nous avons utilisé divers bibliothèque de code spécifique à chaque cas de figure.

\subsection{Développement d'application web complète pour IMPatienT}
Dans le cadre du développement de notre application web et base de données \gls{impatient} nous avons entièrement construit un application web de zéro. Les applications web sont composées de trois éléments: l'interface (nommée \textit{front-end}), le serveur de calcul (nommé \textit{back-end}) et la base de donnée. Pour construire l'interface de \gls{impatient}, nous avons utilisés la bibliothèque de code graphique CSS BootStrap (\cite{mark_otto_bootstrap_2011}) couplée à la bibliothèque de code javascript JQuery (\cite{resig_jquery_2006}) pour l'intéractivité du site. Concernant la partie serveur, nous avons utilisé la bibliothèque de code python nommée Flask (\cite{ronacher_flask_2010}).  Nous avons utilisé Flask car c'est une bibliothèque minimaliste et bien documentée qui permet constuire un site web rapidement en augmentant graduellement la complexité. Enfin en guise de base de données, nous avons utilisé le système de base de données relationnelle SQLite \cite{hipp_sqlite_2020}. Le système SQLite a l'avantage d'être léger et portable car la base de données n'est composée que d'un seul fichier tout en proposant presque les même fonctionnalités que les système de base de données relationnelles plus complexes.

\subsection{Développement d'outil en ligne de commande pour MyoQuant}
Dans le cadre du développement de notre outils d'analyse d'images \gls{myoquant}, nous avons décidé de rendre l'outil disponible d'abord sous la forme d'outil en ligne de commande. En effet, comme \gls{myoquant} est voué à analyser des images de grande taille pendant des temps d'analyse longs et sur des ordinateurs avec de l'équipement spécialisé, l'outil en ligne de commande semble être la modalité la plus adaptée. Pour cela nous avons utilisés les bilbiothèques de code python nommée Typer (\cite{ramirez_typer_2019}) et Rich (\cite{will_mcgugan_rich_2020}). Typer permet de créer des commandes complexes pour un outil en ligne de commande, de valider les paramètres et de générer automatiquement le documentations nécessaire à l'outil, tandis que Rich permet d'enrichir l'expérience de l'usage de la ligne de commande en affichant des données formatée directement dans le terminal (en couleur, sous forme de tableau, interactives etc...).

\subsection{Développement de démonstration en ligne pour NLMyo et MyoQuant}
Enfin, pour le développement de \gls{nlmyo} mais aussi pour \gls{myoquant} nous avons voulu développé des applications de démonstration en ligne. Ces applications de démonstrations ont pour but de faciliter la communications autour des outils et de pouvoir démontrer leur utilité. Pour cela nous avons utilisé la bibliothèque de code python nommée Streamlit (\cite{adrien_treuille_streamlit_2018}). Streamlit permet de constuire des applications web simples et interactives très rapidement sans avoir à gérer la partie interface et serveur séparément, le tout dans un seul et même language de programmation. L'utilisation de Streamlit nous a permis de mettre en place, de modifier et d'affiner rapidement des démonstration de nous outils basés sur l'IA.

\section{Recherche ouverte et reproductibilité}
La recherche ouverte est essentielle notamment en \gls{ia} pour faciliter l'adoption des outils, les améliorer et permettre leur analyse critique. Dans cette optique de science ouverte nous avons utilisé différents outils pour partager mes travaux de recherche de façon libre et ouverte et les rendre reproductibles.

\subsection{Développement open-source et versionnage avec GitHub}
Le code correspondant à l'ensemble des outils et des travaux présenté dans cette thèse sont disponible de façon libre et open-source sur mon profile GitHub (\href{https://github.com/lambda-science}{https://github.com/lambda-science}). GitHub est une société fondée en 2008 qui permet le versionnage de code informatique. Le versionnage est un système qui permet de traquer les modifications apportée  au code dans le temps et de garder une trace de toute les version qui ont existé. L'utilisation de tel système facilite la collaboration en permettant à n'importe qui de contribuer au code, de proposer des améliorations ou de signaler des problèmes éventuels.

\subsection{Développement de données et modèles IA open-source avec HuggingFace}
Si GitHub permet de versionner le code, il est aussi necessaire d'avoir des outils permettant de rendre publique et versionnable les modèles d'\gls{ia} entrainés et les données utilisée. Mettre à disposition les modèles IA et les données utilisée de façon open-source permet à la communauté d'évaluer de manière indépendantes les modèles mais aussi de les améliorer en en entrainant de meilleurs à partir du jeu de données de base. Pour cela nous avons utilisé mis en ligne les données utilisée pour entraîner le modèle SDH de \gls{myoquant} ainsi que le modèle en tant que tel avec son code d'entraînement sur la plateforme HuggingFace à l'adresse: \href{https://huggingface.co/corentinm7}{https://huggingface.co/corentinm7}. HuggingFace est une société fondée en 2016 qui permet de mettre en libre des jeu de données et des modèles d'\gls{ia} de manière open-source. De plus, HuggingFace développe de nombreuses bibliothèques de code python open-source spécialisée en \gls{ia} comme la bibliothèque nomée \textit{transformers} qui permet l'entraînement de \gls{llms}.

\subsection{Suivi d'expérience avec \textit{Weight and Biais}}
L'entraînement d'un modèle \gls{ia} est un processus long qui passe par de nombreuses phases de recherche sur les algorithmes et les architectures les plus performantes, les paramètres optimaux ou le meilleure pré-traitement des données pour obtenir  un modèle performant. Pour traquer les performances des divers modèles entrainés, des outils existent à l'instar de GitHub et HuggingFace. Dans cette thèse j'ai utilisé l'outil nommé \textit{Weight and Bias} (\href{https://wandb.ai/}{https://wandb.ai/}), une société fondée en 2017, pour traquer les performances des divers entraînements des modèles de \gls{myoquant} et \gls{nlmyo}. Dans ces chapitres un lien vers les résultats en ligne est fournis pour à l'esembles des informations enregistrées.

\subsection{Environnement de développement reproductible}
Le dernier élément concernant la reproductibilité des travaux après avoir fournis le code, les données et les modèles de manière open-source reste de s'assurer que le code puisse s'exécuter correctement quel que soit l'environnement informatique. Pour cela, pour chaque outils, nous fournissons deux éléments. En premier nous fournissons un environnement virtuel python avec Poetry (\href{https://python-poetry.org/}{https://python-poetry.org/}) qui permet de spécifié les versions exactes de bibliothèques de code python utilisées ainsi que les version de leur dépendances. De plus, nous construisons une image Docker (\href{https://www.docker.com/}{https://www.docker.com/}) pour chaque outils ce qui permet non seulement de contrôler la version de python utilisée mais aussi le système d'exploitation et les versions de ses divers bibliothèques de codes annexes. L'utilisation d'image Docker couplé à un environnement virtuel python spécifique permettent de s'assurer que notre code fonctionne et est reproductible sur n'importe quel matériel informatique.

\subsection{Archivage du code, des données et des résultats avec Zenodo}
Zenodo  (\cite{european_organization_for_nuclear_research_zenodo_2013}, \href{https://zenodo.org/}{https://zenodo.org/}) est un outil développé par le \gls{cern} en 2013, qui permet d'archiver des travaux de recherches, du code et des données et d'y associer un identifiant unique citable (\gls{doi}). Dans le cadre de cette thèse, nous avons crée une archive Zenodo qui contient l'ensemble des éléments de cette thèse, c'est-à-dire: le manuscrit, le code des outils, les données non-confidentielles utilisées, les modèles entraînés et les résultats. Le lien vers cette archive est \href{LIEN A GENERER}{LIEN A GENERER}.