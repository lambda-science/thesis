\chapter{ \textit{Big Data}, données biomédicales et \textit{machine-learning}}

Hiding within those mounds of data is knowledge that could change the life of a patient, or change the world
- Atul Butte, 2012

L'informatisation du monde a permis la production et l'accumulation de données de façon exponentielle. En particulier dans le domaine de la santé, les avancées technologiques comme les technologies de séquençage, d'imagerie ou les dossiers médicaux électroniques ont permis au cours du temps la capture de données précieuses pouvant améliorer le parcours et la prise en charge des patients.

Dans le même temps, les technologies d'analyse de données massives (\textit{Big-Data}) se sont développée notamment grâce au \gls{ml}, une branche de l'\gls{ia} permettant à des algorithmes informatiques d'apprendre à partir des données. Cette massification des données biomédicales et le développement des technologies d'analyse de données permet d'entrevoir un monde où les soins de santé pourraient être personalisés, préventifs et prédictifs .

Dans ce premier chapitre d'introduction nous présenterons d'abord ce que sont les big-data, la variété des données-biomédicales et comment ces données sont utile pour une meilleure prise en charge des patients.. Puis dans une seconde partie, nous présenterons les concepts principaux du \gls{ml} pour le traitement de ces données.

\section{Les données biomédicale: des \textit{Big Data} au service des patients}

\subsection{Définiton du \textit{Big-Data}}

Le terme "Big Data" est utilisé pour faire référence à l'immense quantité de données complexes et hétérogènes produites par cette informatisation du monde et le développement des technologies haut débits (\cite{de_mauro_formal_2016}). La définition des big-data a été enrichie au fur et à mesure des années d'abord par 3 mots clés puis 5 et même jusqu'à 7 (\cite{garcia_what_2022}). La définition la plus commune aujourd'hui des Big-Data se compose des 5 "V": le volume, la variété, la vélocité, la véracité et la valeur (\cite{ishwarappa_brief_2015}). Ainsi pour être considérée comme Big-Data, les données doivent: (i) être volumineuse, du gigabytes à l'exabytes, (ii) être variée c'est à dire multimodales (iii) avoir une vélocité de création et de traitement importante, (iv) être vérace, c'est à dire valide et (v) avoir une forte valeur ajoutée, c'est-à-dire qu'elle doivent être utiles. (\cite{garcia_what_2022}).

Ainsi les données biomédicales sont en adéquation à cette définition (\cite{zheng_application_2021}). Grâce aux améliorations en techniques d'acquisitions (séquençage, imagerie) elles sont volumineuses et possèdent une forte vélocité. Ces données sont variées, contenant des informations sur le plan génétique, phénotypique et histologique. De plus elle ont un véracité assurée couplée à une forte valeur ajoutée. En effet ce sont de manière général des données générée par des experts (médecin ou biologiste) et liées à des patients (et donc fortement valorisées). Il est alors juste de parler de qualifier les données biomédicales de Big-Data (\cite{sonawane_network_2019}. Ainsi dans la prochaine section nous allons voir en détails les différents modalités de données biomédicales de patients, leur acquisitions et les challenges que présentent leur analyse.

\subsection{Variété des données biomédicales}
Les données biomédicales sont par nature multimodales (\cite{acosta_multimodal_2022}). Le diagnostic d'un patient peut se réaliser par l'intégration de différents niveau d'informations. Tout d'abord il y a les données phénotypiques, listant les symptômes et autre caractéristiques du patient après un examen médical. Ces données sont souvent sous la forme de texte libre, rédigé par le praticien de santé. Ensuite les données d'imagerie, issues le plus souvent d'examens complémentaire pour mieux caractériser l'atteinte du patient (échographie, IRM, histopathologie). Et enfin il y a les données génétiques et omiques, qui sont nécessaires dans le cadre de maladie génétiques pour cibler les dysfonctionnement d'origine génétiques (données de types séquences). Ce trio texte libre, imagerie et séquences implique des techniques d'acquisition, de traitement et des difficultés propres.

\subsubsection{Données textuelles et dossiers de santé électroniques}
Les données en texte libre sont très commune dans le cadre des données de santé. Un rendez-vous, un examen complémentaire ou un échange entre confrères, peuvent donner lieu à la rédaction de comptes rendus médicaux en texte libre contenant des informations expertisées et hautement valorisée concernant le diagnostic du patient. De plus, la rédaction de comptes-rendus ne nécessite pas de technologie d'acquisition particulière, ce qui a permis l'accumulation au cours du temps d'archive de comptes-rendus médicaux massive qui restent à explorer.

Dans le cadre de cette volonté d'explorer ces données, les \gls{dse} sont des outils pour numériser ces comptes-rendus textuels afin de les centraliser et les exploiter. Cependant le développement d'outils pour numériser et exploiter les comptes rendus en texte libre est difficile. La compréhension du texte libre par un programme informatique est une tâches ardue. C'est pourquoi la majorité des solutions de \gls{dse} demandent une phase d'annotation manuelle (remplissage de formulaires et de champs) par l'utilisateur pour numériser les données, ce qui est pratique est rarement réalisé faute de temps.

\subsubsection{Données d'imagerie: microscopie à haute résolution}
Le développement des techniques d'imagerie a permis une diversification des techniques d'imagerie (IRM, échographie, microscopie optique et électronique, imagerie 2D et 3D...) tout en améliorant leur résolutions et précision de capture  et en réduisant les coûts associés (\cite{abdallah_history_2017, prakash_super-resolution_2022, sheppard_structured_2021}). Ainsi l'imagerie médicale est devenue un examen de routine pour le diagnostic de divers pathologies. Cette production de données d'imagerie en routine et de grande résolution ont donné lieu à une massification des données d'imagerie. Ainsi il peut-être pour un clinicien d'évaluer manuellement ces données de manière exhaustive. Le développement d'outils capable d'analyser et de quantifier les éléments d'intérêt sur les données d'imagerie est donc un enjeu majeur (\cite{tchito_tchapga_biomedical_2021}) pour à la fois accélérer l'évaluation des données d'imagerie mais aussi pour améliorer la précision des cliniciens. Par exemple, dans le cadre de la microscopie photonique, il est maintenant courant d'utiliser des scanner de lame complète, générant ainsi des images à l'échelle du gigabytes par lame. Ces images sont extrêmement coûteuses en temps s'il on veut réaliser une évaluation manuelle exhaustive et un comptage des caractéristiques pathologiques en vue d'un diagnostic.

\subsubsection{Données génétiques et omiques}
Enfin, les progrès en termes de technologies de séquençage grâces notamment aux technologie de seconde génération à lecture courtes (par exemple Illumina) et aux technologie de troisième générations à lecture longue (par exemple PacBio et Nanopore) ont permis l'accès à l'ensemble des informations génétiques et omiques de l'Homme. De plus la baisse des coûts de séquençage, rend possible l'utilisation de séquençage de génome complet pour le diagnostic de maladies génétique chez les patients. Plus récemment, les techniques dites "omiques" sont utilisée pour mieux comprendre les pathologiques tels que les technologies de tanscriptiomiques (expression ARN des gènes), protéomiques (expression protéiques des gènes) et métabolomiques (étude des métabolites). Ces technologies d'obtenir une vue globales de mécanismes biologiques qui opèrent au sein d'un tissus. 

Les données de séquençages sont massives, le génome humaine mesurant environ 3.1 miliards de paires de bases (bp), le séquençage d'un génome unique avec une profondeur de 50X (nécessaire pour la détection de mutation génétique) représente un minimum de 150 miliards de paires de bases lues et stockées, pour un individus. Ces données massives de type séquence, requiert des outils spécifique et un matériel informatique adapté à leur traitement. De plus la détection de mutations pathogène reste un challenge

Les données biomédicales, grâces aux nouvelles technologie haut-débit en imagerie, en séquençage en et \gls{dse} sont donc des big-data dont l'aquisitiion est utile aux patients



\subsection{Utilité biomédicales pour les patients}
Stratégie NHS + key for discovery science

\subsection{Les challenge de l'analyse des données biomédicales}
Utiliser les données biomed = cool pour patient mais COMMENT car justemlent y'en a trop ?
Défi d'analyse des big-data manuel impossible, utiliser l'IA tradit pour le traitement !

Requirements of Health Data Management Systems for Biomedical Care and Research: Scoping Review

\section{Machine-Learning pour le traitement des données biomédicales}
\subsection{Les données, lexique}
jeu de données à présenter pour mettre en évidence tabularité, train val test

\subsection{Différentes taches: classification, régression, clustering}
\subsubsection{IA de classificaiton}
\subsubsection{IA de régression}
\subsection{Différentes méthodes d'apprentissages}
\subsubsection{Apprentissage supervisé}
\subsubsection{Exemple d'applicaiton dans la gradation des mutations}
\subsubsection{Apprentissage non-supervisé}
\subsubsection{Apprentissage par renforcement}
\subsection{Différents algorithmes et explicabilité}
\subsubsection{Le concept d'explicabilité}
\subsubsection{Méthodes à bayesiennes}
\subsubsection{Méthodes à base d'arbres}
citer MISTIC
\subsubsection{Systèmes de classeurs}

\subsection{Limites du ML aux données biomédicales}
traitement manuel des données 
Data Integration Challenges for Machine Learning in Precision Medicine