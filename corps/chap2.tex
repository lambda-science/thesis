\chapter{Réseaux de neurones pour le traitement de données biomédicales non-structurées}

\epigraph{\LARGE{``People should stop training radiologists now. It's just completely obvious that in five years deep learning is going to do betther than radiologists.''}}{\LARGE{-- Geoffrey Hinton, 2016}}


La vision de Geoffrey Hinton, lauréat du prix Turing 2018 pour ses travaux en \gls{ia} et en \gls{dnn}, était peut-être un petit peu optimiste. Presque huit ans après cette prédiction, les radiologues n'ont pas été remplacés par l'\gls{ia} et continuent d'être formés. Cependant il est important de remarqué que les méthodes \gls{ia} ont fait des progrès considérables et peuvent présenter des performances similaire aux radiologues, par exemple dans l'évaluation de radiographie de poumons (\cite{frauke_rudolf_ai_2023}). En juin 2023, il y a 238 produits médicaux basé sur IA pour la radiologies et autre méthodes d'imageries avec autorisation de mise sur le marche par la \gls{fda} (\cite{keith_j_dreyer_acr_2023}). Si l'\gls{ia} n'est pas encore prête à remplacer les radiologues et praticiens dans l'évaluation des données d'imagerie, elle est au moins maintenant capable de les assister afin de permettre un gain de temps et de précisions dans l'évaluation des données.


Au cours de la dernière décénie, le domaine de l'\gls{ia} a été révolutionné par l'apparaition des réseaux de neurones profonds (\textit{Deep Neural Networks}, DNN) grace notamment aux travaux des Yann Lecun, Geoffrey Hinton et Yoshua Bengio. Cette nouvelle technologie d'\gls{ia} fait une promesse intéressant dans le cadre des données biomédicales: être capable de traiter automatiquement des données non-structurée, c’est-à-dire sans devoir définir des descripteurs pertinents manuellement. Il est donc tout à fait pertinent d'explorer comment ces modèles peuvent être exploités pour le traitement des données biomédicales multimodales et hétérogènes. Dans ce chapitre nous allons d'abord présenter le fonctionnement des réseaux de neuronnes. Puis nous allons étudier deux architectures spécifiques de réseau de neuronnes qui ont permis la création de modèles d'analyse d'images (réseaux convolutifs)  et de texte libres (réseaux de types transformers).

\section{Présentation des réseaux de neuronnes profonds}

\subsection{Le concept de neurone et réseaux de neuronnes profonds}
Les réseaux de neurones sont un concept ancien qui ont été décrit pour la première fois en 1958 par Frank Rosenblatt (\cite{rosenblatt_perceptron_1958}) sous sa plus simple forme nommée le perceptron, un réseau composé d'un seul neurone formel. Les réseaux de neurones reposent sur le concept bio-inspiré des neurones. La figure \ref{fig:neurons} présente les similarité entre un neurones biologiques et un neurones formel en \gls{ia}. Le neuronne biologique via des processus biochimiques. Les signaux d'entrés des neuronnes précédants sont captés via les dendrites, intégré dans le corps cellulaire pour transmettre ou non un signal de sortie à travers l'axone vers d'autres neurones. Le neurone formel d'\gls{ia} est un modèle simplifié du neurone biologique qui mime leur fonctionnement. Ainsi le neurones formel intègre des entrées (x1, x2, x3 sur le schéma) comme les dendrites, calcul un signal à transmettre (via la somme pondérée des entrées et la fonction d'activation) comme le corps cellulaire et transmet ce signal aux neurones suivant (sortie) comme les axones.

Le perceptron, réseau de neuronne composée d'une seule couche de un ou plusieurs neuronnes entre l'entrée et la sortie, n'est efficace que pour traiter des problèmes à séparation linéaire. Pour traiter des problêmes de classification plus complexes, il est nécessaire de multiplier les couches de neuronnes entre l'entrée et la sortie du réseaux. Ces couches sont appelée couches "cachées", on a ici ce qu'on appelle un réseau de neuronnes profond. La limite entre réseaux de neuronnes classique (perceptron multi-couche) et réseaux de neuronnest profonds est floue. Certains auteur peut considérer un réseau de neuronnes comme profonds à partir de 3 couches cachées, d'autres 10 voir une centaine pour certains.

Similairement au cerveaux, ces neurones formels (artificiel) sont présent en grand nombre dans les réseaux de neurones profonds et sont interconnectés selon une organisation précise, cette organisation se nomme l'architecture du réseau.
\begin{figure}[!htbp]
 \centering
 \includegraphics[width=0.7\textwidth]{figures/neuronne.png}
 \caption[Comparaison du neurone biologique et du neurone formel]{Comparaison du neurone biologique et neurone formel. (A) Représentation schématique du neurone biologique. (B) Représentation schématique du neurone formel utilisé en \gls{ia}}
 \label{fig:neurons}
\end{figure}
\subsection{Les réseaux de neurones: une diversité d'architectures}
La multiplication du nombre des neurones dans un réseau de neurones et de leur interconnection permet de constituer des architectures spécifique qui confère des compétences particulière au réseaux de neuronnes tels que l'intégration d'information locale (pour l'imagerie par exemple grace à la convolution, \cite{fukushima_neocognitron_1980}), l'intégration d'information globales (pour l'analyse de séquence par exemple grace aux \textit{transformers}, \cite{vaswani_attention_2017}), des mécanismes de "mémoires" (pour l'analyse de texte par exemple grace à la \textit{Long short-term memory}, LSTM, \cite{hochreiter_long_1997}) ou encore des capacités génératives (pour la génération d'images par exemples grace aux GAN et aux modèles de diffusion). La figure \ref{fig:dnn_archi} (\cite{leijnen_neural_2016}) présente graphiquement quelques architecture de réseaux de neurones communes. On y retrouve le perceptron (P), le perceptron multi-couches (DFF), le réseau LSTM, le \gls{dnn} convolutifs (DCN) et les réseau antagoniste génératif (GAN).

\begin{figure}[!htbp]
 \centering
 \includegraphics[width=0.9\textwidth]{figures/dnn_archi.png}
 \caption[Représentation de différentes architectures de \gls{dnn}]{Représentation de différentes architectures de \gls{dnn} (modifié de \cite{leijnen_neural_2016})}
 \label{fig:dnn_archi}
\end{figure}
multi layer perceptron d'abord le plus simple puis... convolution, transfermer, diffusion

\subsection{L'entrainnement d'un réseau de neuronnes}
L'entraînement d'un réseau de neurones consiste à trouvers pour chaque neuronne qui le compose, quel est la valeur de poids ($\omega$) pour chaque entrée optimale pour remplir la tâche qui lui est assignée. Pour cela un reseau de neuronnes a besoin de deux éléments: (i) une fonction de coût permettant d'évaluer son niveau d'erreur de classification et (ii) une méthode qui permet de modifier les poids des connections neuronales pour réduire cette erreure grace à la descente de gradient.

\subsubsection{Fonction de coût}
La fonction de coût permet de fournir une mesure quantitative des performances d'un réseau de neuronnes profonds. Elle mesure la divergence entre les prédiction réalisée par le modèle et les labels véritables des données. Cette fonction varie en fonction de la tâche à réaliser, pour une tâche de régression un choix courant est l'utilisation de l'erreur quadratique moyenne (\textit{mean square error}, MSE). Pour une classification il est commun d'utiliser l'entropie croisée binaire (\textit{binary cross entropy}). 
Dans tout les cas, plus cette valeur est élevée plus les prédictions du modèle divergent de la vérité de terrain, plus cette valeure est proche de 0 plus les prédictions du modèle sont exactes. Ainsi, l'objectif de l'entrainnement d'un réseau de neuronnes est de faire converger la valeur de la fonction de cout du jeu d'entrainnement et du jeu de validation vers 0. 

La figure\ref{fig:loss_func} présente un example théorique de valeur de fonction de coût au cours de l'entraînement d'un modèle \gls{dnn}. Dans cet exemple l'écart tout au long de l'apprentissage entre le jeu de validation et d'entrainnement est important à des fin de visualisaiton, en pratique les deux courbes doivent presque se superposer. On observe ici qu'au cours de l'entrainnement cette valeur converge vers 0. Cependant au bout d'un moment la valeur pour le jeu d'évaluation remonte tandis que celle du jeu d'entrainnement continue de baisser, il s'agit du phénomène de sur-apprentissage, le modèle cesse d'apprendreà généraliser et apprend simplement par coeur le jeu d'entrainnement. Ainsi il est necessaire d'arrêter l'apprentissage au moment ou la valeur du jeu de validation augmention, il s'agit de ce qu'on appelle l'arrêt prématuré pour éviter le sur-apprentissage.
\begin{figure}[!htbp]
 \centering
 \includegraphics[width=0.7\textwidth]{figures/loss_function.png}
 \caption[Schéma d'un exemple de fonction de côut lors d'un entrainnement]{Schéma d'un exemple de fonction de côut lors de l'entrainnement d'un \gls{dnn}}
 \label{fig:loss_func}
\end{figure}
\subsubsection{Rétropropagation et descente de gradient}
La figure \ref{fig:retropop} (\cite{scalzitti_nouvelle_2021}) présente le mécanisme de rétropropagation. Lors d'une prédiction, les signaux sont propagés vers l'avant (nommé \textit{forward pass}) à partir de la couche d'entrée jusqu'à la couche de sortie, puis l'erreur est ensuite calculée grace à la fonction de coût. Lors de l'entrainnement, le chemin inverse est réalisé en propageant le gradient de l'erreur pour identifier les neuronnes responsable des erreurs (\textit{backward pass}) (\cite{lecun_deep_2015}). Ce processus de rétropapagation permet d'identifier quels sont les neuronnes responsable des erreurs dont les paramètres doivent être modifié pour réduire la fonction de coût.
\begin{figure}[!htbp]
 \centering
 \includegraphics[width=0.7\textwidth]{figures/retro_propagation.png}
 \caption[Schéma de la rétro-propagation]{Schéma de la propagation du signal lors de la prédiction et de la rétro-propagation lors de l'entraînement (\cite{scalzitti_nouvelle_2021})}
 \label{fig:retropop}
\end{figure}
Après avoir identifier les neuronnes à modifier (et donc avoir calculé le gradient de chaque neuronne), leurs poids vont être ajusté grâce à la méthode de la descente de gradient. La figure \ref{fig:grad_descent} présente le fonctionnement de la descent de gradient. A chaque étape d'apprentissage les poids des neuronnes (ici un seul poids pour un neuronne sur la figure) sont mis à jour dans la direction négative du gradient, ce qui a pour effet de réduire la valeur de la fonction de coût. Cette modification des poids  proportionelle d'un paramètre nommé le pas d'apprentissage (\textit{learning rate}). Ces cycles d'apprentissage vont se répéter jusqu'à atteindre un minimum (global ou local) dans la fonction de coût, indiquant un poids optimal pour la fonction de côut (et donc la tache à réaliser).
\begin{figure}[!htbp]
 \centering
 \includegraphics[width=0.7\textwidth]{figures/gradient_descent.png}
 \caption[Schéma de la descente de gradient pour un poids d'un neuronne]{Schéma de la descent de gradient pour un poids d'un neuronne}
 \label{fig:grad_descent}
\end{figure}

La rétroprogation et la descente de gradient permettent l'apprentissage de réseaux de neuronnes composés de centaines de miliers voir de milions et même de miliards de neuronnes. La taille des \gls{dnn} est très variable en fonction des architectures et de la tâche à effectuer. Ainsi les couts en calcul pour leur entrainement (le calcul des gradients et la mise à jours des poids) peut devenir très importante. Dans la prochaine sous-section nous allons plus réseaux et architectures de \gls{dnn} et les ressources informatiques associées nécessaire à leur utilsation.

\subsection{Nombre de paramètres et ressources informatiques}
Il existe un lien de proportionnalité entre la taille (en nombre de neurones) d'un \gls{dnn}, sa précision et les ressources de calculs nécessaires à son entraînement et inférence. En théorie plus un réseau est grand, plus sa capacité à capturer des relations complexes entre les données est importante et donc il peut atteindre une meilleure précision. Le tableau \ref{table:dnn-size} (\cite{chollet_keras_2023}) présente plusieurs architectures utilisée pour la classification d'image. On observe que pour une même architecture, augmenter le nombre de paramètres permet d'obtenir de meilleurs performances mais au cout d'un temps d'inférence plus long. Cependant lors d'une comparaison de deux architecture différentes, la relation performance - nombre de paramètre n'est pas forcément vérifiée. Les réseaux EfficientNet performent mieux que les réseaux ResNet avec un nombre de paramètre moindre, cependant une règle qui reste vérifiée dans toute les conditions: les réseaux qui performent le mieux ont un temps d'inférence plus long, quelque que soit leur architecture.

\begin{table}[!htbp]
\centering
\begin{tabular}{|l|c|c|c|} 
 \hline
 Architecture & Paramètres (Milions) & Précision (ImageNet, \%)  & Temps d'inférence (ms) \\
 \hline
MobileNetV2 & 3,5 & 71,3 & 3,8 \\
\hline
ResNet50V2 & 25,6 & 74,9 & 4,6 \\ 
ResNet101V2 & 44,7 & 77,2 & 5,4 \\ 
ResNet152V2 & 60,4 & 78,0 & 6,6 \\
\hline
EfficientNetB1 & 7,9 & 79,1 & 5,6 \\
EfficientNetB2 & 9,2 & 80,1& 6,5 \\
EfficientNetB3 & 12,3 & 81,6 & 8,8 \\
 \hline
\end{tabular}
\caption{Tableau de comparaison d'architecture de \gls{dnn} et performances (\cite{chollet_keras_2023})}
\label{table:dnn-size}
\end{table}

L'entrainnement et l'inférence des \gls{dnn} se réalise sur un matériel informatique spécialisé nommé \gls{gpu}. Au contraire des processeur (CPU) qui sont conçu pour des opératins séquentielles, les \gls{gpu} permettent de réaliser un grand nombre d'opération mathématiques en parallèle. Or, les opération principales nécessaires pour l'entraînement et l'inférence d'un \gls{dnn} sont des calculs matriciel et donc intrinsèquement parallélisables. La caractéristiques limitant des \gls{gpu} concerne leur mémoire disponibles (VRAM). En effet, plus le modèles de \gls{dnn} est grand, plus il possède de paramètre, plus il va est demandeur en mémoire, car il est necessaire de pouvoir stocker l'ensemble des poids à optimiser en mémoire. 

Ainsi, s'il est possible d'entrainer et de faire de l'inférence de modèle de taille raisonnable sur des \gls{gpu} accessibles au grand public, cette tâche devient complexe voir impossible pour des modèles de très grande taille sans engager des coûts de plusieurs centraines de miliers d'euro de matériel. A titre d'exemple, il est possible d'héberger et d'entrainnemer des modèles de quelques dizaines de milions de paramtères à quelques miliards paramtères (tel que Resnet50 (25M paramtères) et LLaMA-7B (7 miliards de paramtères) sur un seul \gls{gpu} grand public (16 à 24 Go de mémoire). 

Cependant plus récemment avec le développement des \gls{llms}, des modèles de plusieurs dizaine voir centaines de miliards de parmètres ont été entrainé et sont utilisé, comme par exemple GPT-3 d'OpenAI (175 milliard de paramtères \cite{brown_language_2020}) ou LLaMA de META (65 miliards de paramètres \cite{touvron_llama_2023}). Ce type de modèles demande l'utilisation de plusieurs \gls{gpu} haute de gamme (4 à 8) pour leur hébergement et inférence. A titre indicatif, un cluster composé de 4 \gls{gpu} H100 (40,000€ pièce), coûte 17.9\$/heure d'utilisation, soit environ 13 000\$ d'hébergement mensuel pour un modèle accessible en continu. Un travail d'optimisation pour réduire la taille des modèles tout en conservant leur performances est donc nécessaire pour permettre d'utiliser ce type d'architectures et d'outrepasser les challenges liés à leur hébergement.

\section{L'analyse d'imagerie par réseau neuronnal convolutifs}
L'architecture des réseaux de neuronnes convolitifs (CNN) permettent l'analyse et l'exploitation de données de types images brutes. Cette architecture est basée sur les travaux réalisée sur le cortext visual de chats et de primates dans lesquels il a été démontré que chaque neuronnes du cortext visuel captaient une information locale, dans un champs visuel réduit. Deplus chaque neuronne n'est en capacité de capter qu'une orientation de ligne (horizontale, verticales ou obliques) (\cite{hubel_receptive_1959, hubel_single_1959}). A partir de ces travaux, le concept de couches convolutive pour les réseaux de neurones a été formulé (\cite{fukushima_neocognitron_1980}) et a permis à la construction du premier \gls{dnn} convolutif pour reconnaitre des numéros sur des chèque de banque grace au réseau LeNet-5 (\cite{lecun_gradient-based_1998}). Vingt-cinq ans plus tard, en 2023, les\gls{cnn} sont encore utilisés pour l'analyse de données d'imagerie biomédicales (\cite{holscher_next-generation_2023, ker_automated_2019}).
 
\subsection{Fonctionnement des couches convolutives pour l'analyse d'images}
Pour simuler le comportement des neurones du cortex visuel, les neuronnes d'une couche convolutives ne sont connectés qu'à une zone restreinte d'une image, généralation sous forme d'un carré de pixels. La figure \ref{fig:conv_simple} présente schématiquement la liaison de trois neurones répartis sur deux couches convolutives par rapport à une image de base. On observe que ces neuronnes n'ont accès qu'à une portion de l'image, donc accès à une information locale. 
\begin{figure}[!htbp]
 \centering
 \includegraphics[width=0.7\textwidth]{figures/conv_simple.png}
 \caption[Schéma de la connection des neurones convultifs à une image]{Schéma de la connection des neurones convultifs à une image. Les neuronnes ne sont connectés qu'à une portion locale de l'image sous la forme d'un carré de pixels.}
 \label{fig:conv_simple}
\end{figure}
La convolution consiste à appliquer un filtre à une entrée pour produire une carte de caractéristiques (\textit{feature map}). Ainsi les neurones de la couche convolutive ont en entrée une portion de l'image et vont appliquer un filtre pour extraire une information de cette portion (une ligne horitontale, une texte, un contraste...). Le but de l'entraînement du \gls{cnn} est, pour chaque neuronne, de trouver quels sont les filtres optimaux à utiliser pour extraire l'information pertinente à la classification de l'image. Autrement dis chaque neuronne va apprendre à extraire une information pertinente de la zone à laquelle il a accès.

La figure \ref{fig:conv_simple} est simpliste, car elle représente les couches convolutives comme composée d'une seule couche de neuronnes et donc un seul filtre. En réalité, comme présenté en figure \ref{fig:conv_complex} chaque couche convolutives est composée de plusieurs filtres (couches de neuronnes formant des carte de caractéristique). Chacune de ces \textit{features map} est reliée aux précédentes afin d'extraire des caractéristiques très diverses (des formes horizontales, verticales, de la texture, du contraste...).
\begin{figure}[!htbp]
 \centering
 \includegraphics[width=0.7\textwidth]{figures/conv_complex.png}
 \caption[Schéma des couches convolutives]{Schéma des couches convolutives complexes.}
 \label{fig:conv_complex}
\end{figure}
Enfin pour simplifier les couts de calcul et réduire la mémoire nécessaire, après un bloc de covolution une étape de \textit{max-pooling }(en français: sous-échantillonnage maximal) est réalisée. Cette technique permet de réduire la dimensionalité des carte de caractéristique en préservant les informations essentielles. La figure \ref{fig:max-pool} présente le fonctionnement de cette méthode. La carte de caractéristique est divisée en carré non-chevauchant et la valeur maximal de chaque carrée est sélectionnée. Par exemple si on a une carte de caractéristique de taille 4x4 et qu'on réalise un \textit{max pooling} de taille 2, on obtient une carte de caractéristique de taille 2x2, c'est à dire quatre fois plus petite.
\begin{figure}[!htbp]
 \centering
 \includegraphics[width=0.5\textwidth]{figures/max-pool.png}
 \caption[Technique de max-pooling]{Schéma de la technique de max-pooling pour réduire la dimensionalité d'une matrice.}
 \label{fig:max-pool}
\end{figure}

Pour finir, la figure \ref{fig:cnn_archi} présente la structure typique d'un \gls{cnn}. Un \gls{cnn} consiste en l'enchainnement de couche de convulution et de \textit{max-pooling} avant le couches de classification (\textit{fully connected}). Le but de cette architecture est de réduire la dimensionalité de l'image tout en augmentant la profondeur (c'est à dire le nombre d'informations extraite par position). Ainsi par exemple pour une image de 512x512 (c'est à dire 262 144 pixels), si en sortie de couches convolutives on obtient une matrice de taille (4x4x128, soit 2048 points), cela signifie que nous avons extrait 128 caractéristiques pour chacune des 16 zones de l'image. Et ce sont ces 128 caractéristiques par zone qui vont être utilisée pour classer l'image.
\begin{figure}[!htbp]
 \centering
 \includegraphics[width=0.7\textwidth]{figures/cnn_simple.png}
 \caption[Schéma de la structure d'un \gls{cnn} typique]{Schéma de la structure d'un \gls{cnn} typique}
 \label{fig:cnn_archi}
\end{figure}
\subsection{Modèle grand public pour l'histologie}
La couche de neurones convolutif représente la brique de base des \gls{cnn}. A partir de cet architecture de réseau il est possible de réaliser diverse tache en variant légèrement l'organisation des couches. Dans cette section nous présentons deux exemples d'utilisation des \gls{cnn} pour le traitement de données biomédicales.

\subsubsection{Segmentation d'images histologiques: détection de cellules avec Cellpose}
La segmentation d'une image consister à diviser l'images en groupes de pixels (segment) dans l'objectif d'obtenir les coordonnées d'éléments d'intérêts. Par exemple, dans le cadre d'une image de coupe histologique, il peut être intéressant de mesurer le nombre de cellules présente et leur taille. Pour automatiser ce processus, il est necessaire d'avoir recours à un réseau de neuronne capable de réaliser de la segmentation d'image.
Cellpose (\cite{stringer_cellpose_2021}), développé par Carsen Stringer en 2021, est un modèle de segmentation généraliste, conçu pour être capable de segmenter les cellules de n'importe quelle coupe histologique. La figure \ref{fig:cellpose_archi} présente l'architecture du modèle ainsi qu'un exemple d'image histologique et de résultat de segmentation. L'architecture de\gls{cnn} utilisée se nomme U-Net (\cite{ronneberger_u-net_2015}) structurée comme un U avec un chemin de contracteur (encodeur grâce à la convolution) et un chemin d'expansion (décodeur grace à la "\textit{upconvolution}"). Cette architecture permet à partir de l'image d'entrée, composée de cellules en microscopie à fluorescence, de générer un masque de segmentation, de la même taille que l'image d'entrée. Ce masque de segmentation est abstraction de l'image d'entrée où les pixels de chaque objet (cellules) sont marqués avec un identifiant unique. AInsi il est possible à partir de ce masque de compter ou de mesurer la taille des cellules.
\begin{figure}[!htbp]
 \centering
 \includegraphics[width=0.8\textwidth]{figures/cellpose_archi.png}
 \caption[Architecture du réseau convolutionel de CellPose]{Architecture du réseau convolutionel de CellPose (modifié de \cite{stringer_cellpose_2021})}
 \label{fig:cellpose_archi}
\end{figure}
\subsubsection{Analyse de séquences: prédiction de sites d'épissage avec Spliceator}
En plus du traitement d'images, les \gls{cnn} peuvent être utilisés pour traiter des données de type séquences nucléotidiques. Au sein de notre équipe, en 2021, l'outil Spliceator a été developpé (\cite{scalzitti_spliceator_2021}) pour analyser des séquences génomiques et prédire les sites d'épissages. La figure \ref{fig:splice_archi} présente la structure du \gls{cnn} entrainné pour cette tache de classification. Le \gls{cnn} entrainé est capable de prédire les sites d'épissage de plus de 100 espèces vivantes avec une précision de plus de 90\%. L'architecture utilisée par Spliceator est une architecture classique de \gls{cnn} avec trois blocs convolutifs puis une couche \textit{fully-connected} pour la classification, identique à l'exemple en figure \ref{fig:cnn_archi}. Cette architecture a permis d'entrainer un modèle capable de prendre en contexte un contexte jusqu'à 600 nucléotides en amonts et en aval du site d'épissage à évaluer.
\begin{figure}[!htbp]
 \centering
 \includegraphics[width=0.66\textwidth]{figures/spliceator_nn.png}
 \caption[Architecture du réseau convolutionel de Spliceator]{Architecture du réseau convolutionel de Spliceator (\cite{scalzitti_spliceator_2021})}
 \label{fig:splice_archi}
\end{figure}

Bien que les \gls{cnn} soient capable d'intégrer de l'information locale pour mimer le fonctionnement du contexte visuel dans le contexte d'analyse d'image, cela peut poser problèmes pour la déttection et l'intégrations d'intéraction longue distance. Par exemple dans le cadre d'analyse de séquences, s'il on cherche à prédire l'expression d'un gène, il est connu qu'il existe des signaux à longue distances qui modulent cette expression, tel que les séquences \textit{enhancers}, dont plus de la moitiés sont situé à des distances de 50 000 paire de base ou plus du gène modulé (\cite{chepelev_characterization_2012}). De mê me pour l'analyse de texte, une information importante pour la compréhension d'une phrase peut être située bien en amont à plusieurs dizaine voir centaines de mots.

Ainsi pour constuire des architectures de \gls{dnn} capable de tacler ces analyse de texte et de séquences, il est necessaire d'avoir des modules capable de prendre en compte le contexte global. Ceci est rendu possible grace à l'architecture nommée \textit{transformers} (\cite{vaswani_attention_2017}), qui sont présentés dans la prochaine section.


\section{Architecture transformers et la révolution des modèles linguistiques de grande taille}
Les \textit{tranforsmers} sont des architectures qui ont révolutionner le domaine de l'analyse de texte depuis leur première description (\cite{vaswani_attention_2017}). Cette architecture a permis de dépasser toute les limitations des architectures précédante, en particulier les limites concernant leur faible mémoire à longue distance. Les modèles \textit{transformers} sont des modèles dit "Seq2Seq" c'est à dire séquence à séquence. Ils prennent en entrée une séquence (une phrase ou une séquence protéine) et produisent en sortie une autre séquence (le phrase traduite par exemple). Pour comprendre comment les \textit{transformers} fonctionnent et quels sont leurs implications, deux notions sont nécessaires: la structure encodeur-décodeur et l'attention multi-tête.

\subsubsection{La structure encodeur-décodeur et l'attention multi-tête}
La structure encodeur-décodeur est au coeur de nombreuses architectures et est vitale pour une multitudes de tâches telles que la génération de texte ou la traduction. L'idée derrière cette structure est de compresser l'information d'entrée en un vecteur numérique de taille définie, nommé "état caché" (figure \ref{fig:encoder_decoder}). Puis le décodeur, à partir de cet état caché va générer une séquence (par exemple une phrase) qui correspond à cet état caché. Lors de l'entraînement d'un réseau encodeur-décodeur, les paramtères de l'encodeur et du décodeur sont ajuster pour minimiser la fonction de coût. 

Par exemple, imaginons que l'on veuille construire un réseau de traduction français vers anglais (figure \ref{fig:encoder_decoder}). Pour cela on va utiliser comme donnée d'entraînement des couples de phrase français (entrée) et anglaise (cible). L'entraînement consiste à apprendre à l'encodeur à représenter numériquement les phrases françaises en extrayant les informations les plus pertinente. L'entraînement du décodeur consiste à apprendre comment à partir de cet état caché, reconstruire la phrase anglais attendue.

Cependant cette structure possède des limites. En effet, pour des phrases très longue ou des paragraphes, l'encodeur n'est pas nécessairement en capacité de représenter toute les informations pertinentes dans l'état caché qui possède une taille fixe, ainsi de l'information pourrait être perdue. Les mécanismes d'attention à l'oeuvre dans les \textit{transformers} permettent de pallier à cette limite.
\begin{figure}[!htbp]
 \centering
 \includegraphics[width=0.66\textwidth]{figures/encoder_decoder.png}
 \caption[Schéma de la structure encodeur-décodeur]{Schéma de la structure encodeur-décodeur permettant l'encodage d'une donnée dans un état caché puis son décodage pour la traduction par exemple. }
 \label{fig:encoder_decoder}
\end{figure}

Dans un \textit{transformers} les encodeurs et les décodeurs sont multiples (au total de 6 dans le papier les décrivant pour la première fois) et ils sont composés de mécanismes d'attentions d'auto-attention multi-tête (figure \ref{fig:encoder_attention}). Le mécanisme d'auto-attention permet à n'importe quel élement de la séquence (mot de la phrase) d'être influencé par les autres éléments. C'est-à-dire que le premier mot d'une phrase peut avoir une influence sur le dernier, ce qui permet une influence longue distance. Mathématiquement cela est possible par la création pour chaque mot de la phrase de trois vecteurs: requête (Q), clé (K) et valeur (V) (non représenté sur la figure \ref{fig:encoder_attention}). Puis par un produit scalaire, un score d'attention est calculé pour tout les couples de mot présent dans la phrase (c'est donc un processus quadratique).

Ces mécanismes d'attention sont divisés en plusieurs tête, qui permettent lors de l'entraînement de spécialiser chacune des tête d'attention sur un aspect spécifique de l'entrée. Par exemple dans le cadre d'un texte, une tête peut être spécialisé dans la syntaxe et une autre dans des aspects sémantiques. 
La multiplicité des transformers et les mécanismes d'attention multi-têtes permettent au réseaux de traiter des données de grande tailles sans perte d'infromation, c'est-à-dire avec une grande "mémoire". Ainsi cela permet de détecter et d'intégrer lors des prédictions réalisée des informations à longue distance, là où la convolution ne permettait d'intégrer que des informations locales.
\begin{figure}[!htbp]
 \centering
 \includegraphics[width=0.66\textwidth]{figures/encoder_attention.png}
 \caption[Schéma de la structure encodeur-décodeur d'un réseau transformer]{Schéma de la structure encodeur-décodeur d'un réseau transformer. Chaque encodeur et transformer est composé d'un mécanisme d'attention et d'un réseau à propagation directe.}
 \label{fig:encoder_attention}
\end{figure}
Grâce à cette structure encodeur-décodeur et les mécanismes d'attention multi-tête, il est possible de créer une architecture de \textit{transformer} présentée en figure \ref{fig:transformer}. Cette architectures est composée de deux éléments: à gauche en vert les décodeurs empilé au nombre de 6, qui prennent en entrée la phrase à traduire. A droite en rouge, les décodeurs empilés au nombre de six, prennent à la fois la sortie des encodeurs en compte ainsi que la traduction dont la génération a déjà commencé. Dans chaque décodeur et encodeur on retrouve nos modules d'attentions (en violet). L'inférence de cette architecture est une boucle, les mots sont générés un par un à chaque cycle. Ainsi pour la traduction en anglais de la phrase "Je suis un étudiant" au premier cycle le mot "I" sera généré. Ensuite ce mot va être repris en entrée des décodeurs pour générer le suivant.
\begin{figure}[!htbp]
 \centering
 \includegraphics[width=1\textwidth]{figures/transformer.png}
 \caption[Schéma de l'architecture d'un réseau Transformer]{Schéma de l'architecture d'un réseau transformer. Les transformer sont composés de 6 encodeurs empilés, relié à 6 décodeurs empilés, chacun composés de mécanismes d'attention.}
 \label{fig:transformer}
\end{figure}
Ici pour expliquer le fonctionnement des \textit{transformers} nous avons utilsié l'exemple de la traduction de texte. Mais les \textit{transformers} peuvent être utilisables pour n'importe quelles données séquentielles, telles que les séquences génomiques et protéique. Nous allons voir deux exemples d'utilisation de \textit{transformers} pour l'analysis de séquences biologiques à travers Enformer et AlphaFold.

\subsubsection{Enformer: un transformer pour l'expression des gènes}
Comme écrit précidemment, l'expression des gènes dans le génome est modulée par des interactions longue distance, grace notamment aux \textit{enhancer}. Ainsi s'il on veut constuire des modèles prédictifs de l'expression des gènes il faut avoir une architecture capables de considérer ces intéractions longue distance, c'est précisement l'intérêt des \textit{transformers}. Dans les travaux intitulés "\textit{Effective gene expression prediction from sequence by integrating long-range interactions}" (\cite{avsec_effective_2021}) publié en 2021, une équipe de DeepMind a utilisé un modèle mixant convolutions et transformers pour prédire l'expression des gènes (figure \ref{fig:enformer}. La ou le précédant modèle n'était capable de prendre que 20 000 paires de bases de contexte autour d'un gène pour effectuer sa prédiction, ce nouveau modèle nommé \textit{Enformer} est capable d'étendre le contexte jusqu'à 100 000 paires de bases en amont et en aval du gène. Grace l'examiniation des prédictions réalisée, ils ont réussis à mettre en évidence que le modèle a porté son attention sur la présence d'\textit{enhancer} localisés à 50 000 paires de bases du gène. Ce qui confirme l'intérêt de l'utilisation de transformer dans le cadre de l'analyse de l'expression des gènes et de la prise en compte des \textit{enhancer}.
\begin{figure}[!htbp]
 \centering
 \includegraphics[width=0.8\textwidth]{figures/enformer.png}
 \caption[Schéma de l'architecture du modèle enformer]{Schéma de l'architecture du modèle enformer. Ce modèle basé sur les \textit{transformers} permet de prendre jusqu'à 100 000 paire de base de contexte génomique pour évaluer l'expression d'un gène (\cite{avsec_effective_2021}).}
 \label{fig:enformer}
\end{figure}
\subsubsection{AlphaFold : un transformer pour le conformation 3D des protéines}
En 2020, le modèle AlphaFold 2 (\cite{jumper_highly_2021}) a remporté la CASP14 (\href{https://predictioncenter.org/casp14/}{https://predictioncenter.org/casp14/}), compétition de référence dans le domaine la prédiction de structure protéïque. Leur modèle a obtenu un score global de 244 (somme de z-scores), contre 90,8 pour le second meilleurs modèle (\cite{jumper_applying_2021}). Une des caractéristiques principales du modèle AlphaFold est l'usage d'une architecture similaire aux \textit{transformers} (figure \ref{fig:alphafold}). Ces blocs nommé "\textit{evoformer}" utilisent les même mécanismes d'attention à l'oeuvre dans la structure classique des \textit{transformers}. Comme dans le contexte génomique, les interactions 3D qui ont lieu dans les séquences protéiques ne sont pas que locales, ainsi il est nécessaire de pouvoir prendre en compte des intéractions longues distance pour obtenir une prédiction de qualité, grace à ces mécanismes d'attention. L'utilisation de cette architecture a permis le développement d'un outil qui révolutionne le domaine de la biologie structurale rendant possible l'accès à la structure 3D prédite de n'importe quelle protéine du vivant.
\begin{figure}[!htbp]
 \centering
 \includegraphics[width=1\textwidth]{figures/alphafold.png}
 \caption[Schéma de l'architecture du modèle AlphaFold]{Schéma de l'architecture du modèle AlphaFold. Ce modèle utilise les mécanismes d'attention des transformers pour prédire la conformation 3D de protéines à partir de leur séquences et alignements.}
 \label{fig:alphafold}
\end{figure}

Bien que les \textit{transformers} présente une grand avancée de le domaine des \gls{dnn} avec la prise en compte des intéractions longue distances, ils présentent un certain nombre de limite qui doivent être considérée avant leur utilisation. Tout d'abord en termes de cout de calculs, les transformers par leur mécanismes d'attention (processus quadratric) sont très couteux en mémoire et en ressource pour le traitement de longue séquences. Ensuite, les \textit{transformers} ont besoin d'une quantité de données plus importantes pour leur entrainnement que des architecture moins complexe, ce qui peut être un problême dans le domaine biomédicale (\cite{willemink_toward_2022}). Enfin, alors que pour les \gls{cnn} il existe des méthodes d'explicabilité ad-hoc, les transformers étant plus complexe et récent, les techniques d'explicabilité à diposition sont moindre, mais c'est un domaine recherche actif (\cite{kim_can_2022, saha_explainability_2022}). 

Ainsi l'utilisation des \textit{transformers} est a réserver pour les tâches qui ne sont pas accomplissable par des architectures plus simple. Une de ces taches est la compréhension du language naturel, donc l'architecture \textit{transformer} a permis de révolutionner le traitement grace aux \gls{llms} que nous présentons dans la prochaine section.

\subsection{La ruée vers l'or des modèles linguistiques de grande taille}
Les années 2022-2023 représentent des années clés dans l’histoire du \gls{nlp} avec le développement et la mise à disposition de \gls{llms} généraux performants et accessibles tel que \textit{GPT-3.5-turbo (}souvent nommé à tort \textit{ChatGPT)} ou \textit{LLaMA} (\cite{touvron_llama_2023}).  Ces modèles représente une véritable révolution dans le traitement des données textuelles non structurés car ils sont capables de suivre des instructions précises et extrement variée, dans de multiple langues. Dans un le long papier de 155 pages nommé "\textit{Sparks of Artificial General Intelligence: Early experiments with GPT-4}" publié en avril 2023 par une partie de l'équipe ayant travaillé sur le modèle GPT-4 d'OpenAI (\cite{bubeck_sparks_2023}) les auteurs dressent un portrait des capacités des \gls{llms} et de leur possibles impact sociétaux.

Pour ne citer que quelques exemples des capacités des \gls{llms}, ces modèles sont capable de résumer des articles scientifique en une série de points clés, de générer du code informatiques et d'en corriger les erreurs, d'extraire des informations spécifique d'un texte, de répondre à des questions de logique, d'expliquer des blagues, d'écrire des histoires créatives, d'analyser des images et d'interagir avec le monde extérieur via des requêtes internet. Ces modèles ont donc des capacités extrêment vastes avec des performances proches de l'Homme. Les modèles \gls{llms} sont très divers et en constante évolution. La figure \ref{fig:llm-tree} (\cite{yang_harnessing_2023}) retrace les principaux modèles développés entre 2018 et 2023 où l'on observe une nette évolution entre 2022 et 2023 avec une multitude de modèles ouvert ou fermés développés.
\begin{figure}[!htbp]
 \centering
 \includegraphics[width=1\textwidth]{figures/llm_tree.png}
 \caption[Arbre du développement LLMs de 2018 à 2023 (\cite{yang_harnessing_2023})]{Arbre du développement LLMs de 2018 à 2023 (\cite{yang_harnessing_2023})}
 \label{fig:llm-tree}
\end{figure}
Le développement de tels modèles a été rendu possible par l'utilisation de deux innovations en plus des \textit{transformers}: l'\gls{ssl} et l'\gls{rlhf}.

\subsubsection{Apprentissage auto-supervisé}
En mars 2021, Yann Lecun a écrit un billet de blog qualifiant l'apprentissage auto-supervisé comme la matière noir de l'intelligence \textit{(Self-supervised learning: The dark matter of intelligence}, \cite{lecun_self-supervised_2021}). Dans ce billet il explique que l'apprentissage auto-supervisé est l'une de voies les plus prometteuses pour constuire des connaissances de fond dans les modèles \gls{ia}.


L'idée derrière le \gls{ssl} part d'un constat: l'ensemble des modèles d'\gls{ia} nécessitent un jeu de données bien anotés pour leur entraînement. Or, il est facile d'acquérir un grand nombre de données non-anontés, le goulot d'étranglement se situe dans l'annotation et la labellisation des jeux de données. Par exemple, il est facile de siphoner un grand nombre de page internet et d'en extraire le texte. Comment utiliser ces données non-anontés pour entrainner un modèle de langage ? Est-il possible de définir une méthode d'apprentissage à partir des données non-annotés ? C'est précisement l'intérêt du \gls{ssl}.


Le fonctionnement du \gls{ssl} est relativement simple, dans une phase de pré-entraînement, le modèle va apprendre à ré-créer les données qui lui sont fournies. Par exemple, si l'on veut créer un modèle de langage, on peut lors de ce pré-entrainnement lui fournir une phrase tels que: \\
"Les modèles\gls{ia} sont des systèmes complexes capable de réaliser des prédiciton". \\
Pour le pré-entrainnement du modèle par \gls{ssl} on va cacher certains mots de la phrase en entainer le modèle à prédire le mot attendu. Ainsi on obtient la phrase: \\
"Les modèles\gls{ia} sont des systèmes [?] capable de réaliser des [?]" \\
et pour chaque point d'interrogation, le modèle va être entraîné à prédire les mots "complexes" et "prédictions" avec une grande probabilité. Par cette méthode il est possible d'utiliser des données non-anontées pour réaliser un pré-entainnement qui permet au modèle une meilleure compréhension de la structure d'un texte et de la relation entre les mots. Après ce pré-entrainnement, l'entrainement classique du modèle peut avoir lieu, avec un nombre de données annotées plus limité que sans le pré-entrainnement par \gls{ssl}.


Le concept de \gls{ssl} n'est pas uniquement limité au texte, le principe est similaire avec des images. Par exemple il est possible de diviser une image en 16 zones carrées, de masquer une de ces zone et d'entraîner le modèles à ré-générer la zone en guise de pré-entraînement. Il a été montré sur le jeu d'évaluation ImageNet, que ce pré-entrainenemnt permet d'obtenir de meilleur performances de classifications avec une quantité de données moindre (\cite{goyal_self-supervised_2021})

\subsubsection{Apprentissage par renforcement avec retour humain}
L'apprentissage par renforcement avec retour humain est une méthode qui a permis l'amélioration des performances des \gls{llms} (\cite{ziegler_fine-tuning_2020, stiennon_learning_2020}). Cette méthode permet non seulement de réduire les biais dans le modèle grace à une intervention humaine (biais raciaux, de genre, religieux, \cite{ganguli_red_2022}) mais aussi de créer un modèle capable de suivre des instructions (\cite{ouyang_training_2022}).

Après le pré-entrainnement par \gls{ssl} et l'entrainnement classique du modèle avec des données annotées, les sortie du modèle vont être affinée par apprentissage par renforcement. Plusieurs exemples de générations pour une instruction vont être présenté à un humain qui va devoir classer les réponses dans un ordre de préférence. Le modèle va ainsi obtenir une pénalité ou une récompense pour chaque réponse, ce qui permet d'affiner ses futures réponse. Ainsi le retour humain est intégré à l'entraînement du modèle par ce mécanisme d'apprentissage par renforcement pour rentre le modèle moins biaisé et qualitatif.

\subsection{Les modèles génératifs et modèles d'embedding}
Les \gls{llms} se déclinent sous deux formes principales avec des utilisations différentes: les modèles génératifs et les modèles d'\textit{embedding}. La figure \ref{fig:llm-type} présente les deux types de modèles.
\begin{figure}[!htbp]
 \centering
 \includegraphics[width=1\textwidth]{figures/two_llm.png}
 \caption[Schéma des deux types de LLMs]{Schéma des deux types de LLMs}
 \label{fig:llm-type}
\end{figure}
Les modèles de type \textit{embedding} consistent à transformer notre texte d'entrée en vecteur numérique de grande taille. L'idée derrière cette transformation est que deux phrases ayant un sens similaire (sens sémantique) ont une reprétenstation numérique similaire. Transformer le texte en vecteur numérique permet de réaliser des opérations, tel que de la recherche de similairité entre deux phrases, mais aussi du clustering, de la visualisation graphique ou encore l'entrainnement de modèles de classification. Le concept d'\textit{embedding} est donc un réel couteaux suisse dans pour analyse et rechercher de la similarité entre du texte et il est en général multi-langue: deux phrases identiques mais dans une langue différente vont avoir une représentation numérique similaire. La figure \ref{fig:sentence_embed}  (\cite{luis_serrano_what_2023}) présente une visualisation de l'embedding de 9 phrases en deux dimensions. On observe sur cette figure que les phrases traitant d'un sujet similaire sont regroupée en sous-groupe dans l'espace. On observe un groupe qui concerne les chiens, un groupe concernant le football et un groupe concernant une question de type "comment vas-tu". Cet exemple est simplifié à des fins de représentation car les phrases ne sont encodées qu'en deux dimensions. En réalité pour capturer des nuances complexes dans les phrases, les modèles d'\textit{embedding} encodent les phrases en plusieurs centaines voir milliers de dimensions.
\begin{figure}[!htbp]
 \centering
 \includegraphics[width=0.66\textwidth]{figures/sentence_embed.png}
 \caption[Visualisation d'embedding de 9 phrases en 2 dimensions]{Visualisation d'embedding de 9 phrases en 2 dimensions (\cite{luis_serrano_what_2023})}
 \label{fig:sentence_embed}
\end{figure}
Les modèles génératifs sont des modèles de complétions. A partir d'un texte en entrée ils génèrent un texte de sortie. Ces modèles sont utiles pour des tâches créatives comme la génération d'histoire ou de code, mais aussi pour suivre des instructions précises tel l'extraction d'information d'un texte comme la récapitulation d'un article scientifique. Le texte en entrée contenant les instructions pour le modèle est nommé prompt (figure \ref{fig:llm-prompt}). Le défi dans l'utilisation des modèles génératifs réside dans la création d'un prompt adaptés aux résultats que l'on souhaite obtenir. Le processus de création et d'affinage d'un prompt pour une tâche se nomme "\textit{prompt engineeing}". Classiquement un prompt possède une structure de base en quatre éléments: (i) la description précise de la tâche, (ii) un exemple de réalisation de la tache et de sortie attendu (iii) le texte que l'on souhaite analyser, (iv) un indicateur de sortie indiquant au modèle qu'on attend une réponse. L'avantage des modèles génératifs réside dans le fait qu'aucun apprentissage supplémentaire n'est nécessaire, le point central étant l'établissement d'un prompt optimal.

\begin{figure}[!htbp]
 \centering
 \includegraphics[width=0.66\textwidth]{figures/promt_llm.png}
 \caption[Schéma du concept de prompt pour les LLMs génératifs]{Schéma du concept de prompt pour les LLMs génératifs}
 \label{fig:llm-prompt}
\end{figure}

\subsection{Taille des modèles, hébergement, quantization}
tableau des tailles
papier émergence de capacité taille minimal
https://huyenchip.com/2023/04/11/llm-engineering.html application des LLMs


Ces méthodes permettent d’explorer de façon rétrospective et multimodale, l’ensemble des données biomédicales acquises sur des patients sans avoir besoin de réaliser un travail manuel d’annotation trop important. NEXT : myopathies congénitales notre cas d'application

CONCLUSIONS GENERALE INTRO Into le coeur du propos